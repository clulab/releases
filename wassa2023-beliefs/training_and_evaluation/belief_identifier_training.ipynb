{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from transformers.models.bert.modeling_bert import BertModel, BertPreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "import sys\n",
    "import json\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save models here:\n",
    "general_models_dir = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = \"./logs\"\n",
    "if os.path.exists(logs):\n",
    "    print(\"Dir already exists: \", logs)\n",
    "else:\n",
    "    os.makedirs(logs)\n",
    "    print(\"Created dir: \", logs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "currentDateAndTime = datetime.now()\n",
    "currentDateAndTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONDITIONS\n",
    "\n",
    "setting = \"marked-trigger\" # marked-trigger or unmarked-trigger\n",
    "trigger_source = \"extracted\" # extracted or predicted (predicted not currently used)\n",
    "training_type = \"CV\" # CV or full-model\n",
    "data = \"mturk-clean\" # \"mturk-clean\" or \"mturk-orig\"\n",
    "if data == \"mturk-orig\":\n",
    "    threshold = 0.5 # 1.0 - only count mturk data as belief if all annotators marked it as belief; 0.5 - count as belief if majority marked it a belief\n",
    "transformer_name = \"bert-base-cased\" \n",
    "random_seed=22\n",
    "\n",
    "resample_triggerless = False\n",
    "rewrite_sample = True # if rewrite if sample already exists when trying to sample from a larger set of neg examples\n",
    "if resample_triggerless:\n",
    "    neg_example_multiplier = 4 # num of positive class annotations * neg_sample_multiplier = triggerless sample size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a dir for current experiment\n",
    "specific_dir = f\"{training_type}_{data}_{trigger_source}-{setting}_{transformer_name}_{currentDateAndTime.year}-{currentDateAndTime.month}-{currentDateAndTime.day}-{currentDateAndTime.hour}-{currentDateAndTime.minute}\"\n",
    "if data == \"mturk-orig\":\n",
    "    specific_dir = specific_dir.replace(data, data + f\"-{threshold}-threshold\")\n",
    "models_dir = os.path.join(general_models_dir, specific_dir)\n",
    "print(\"The model will be saved to:\\n\" + models_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(models_dir):\n",
    "    print(\"Dir already exists: \", models_dir)\n",
    "else:\n",
    "    os.makedirs(models_dir)\n",
    "    print(\"Created dir: \", models_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_data = \"\"\n",
    "# can be a big set to sample from or a set to use as is\n",
    "triggerless_sample = \"\"\n",
    "\n",
    "# if resampling, set the directory to save the new sample\n",
    "if resample_triggerless:\n",
    "    # triggerless samples actually used\n",
    "    triggerless_samples_used = \"\"\n",
    "    if os.path.exists(triggerless_samples_used):\n",
    "        print(\"Dir already exists: \", triggerless_samples_used)\n",
    "    else:\n",
    "        os.makedirs(triggerless_samples_used)\n",
    "        print(\"Created dir: \", triggerless_samples_used)\n",
    "else:\n",
    "    print(f\"Using previously sampled data from {triggerless_sample}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting some vars\n",
    "if data == \"mturk-clean\": \n",
    "    meaningful_columns = [\"paragraph\", \"sentence\", \"approx_span\", \"trigger\", \"quality_controlled\"]\n",
    "elif data == \"mturk-orig\":\n",
    "    meaningful_columns = [\"paragraph\", \"sentence\", \"approx_span\", \"trigger\", \"quality_controlled\", \"accepted_count\", \"belief_ann_count\"]\n",
    "else:\n",
    "    print(\"Unknown data type: \", data)\n",
    "    \n",
    "    \n",
    "annotations_column = \"quality_controlled\"\n",
    "neg_sample_meaningful_columns = [\"sentence\", \"paragraph\"]\n",
    "mention_span_column = \"approx_span\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load annotated data\n",
    "adf = pd.DataFrame()\n",
    "for file in listdir(annotated_data):\n",
    "    print(file)\n",
    "    if file.endswith(\"tsv\"):\n",
    "        f_path = os.path.join(annotated_data, file)\n",
    "        temp_df = pd.read_csv(f_path, sep='\\t', usecols = meaningful_columns).dropna()\n",
    "        adf = pd.concat([adf, temp_df])\n",
    "print(\"Annoted data size: \", len(adf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minor clenanup\n",
    "adf[\"sentence\"] = [s.strip() for s in adf[\"sentence\"]]\n",
    "adf = adf.drop_duplicates(subset = [\"sentence\", \"approx_span\"])\n",
    "print(\"Annoted data size (updated): \", len(adf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add labels\n",
    "if data == \"mturk-orig\":\n",
    "    adf[\"prop\"] = adf[\"belief_ann_count\"]/adf[\"accepted_count\"]\n",
    "    adf[\"label\"] = [1 if x >= threshold else 0 for x in adf[\"prop\"]]\n",
    "    adf = adf.drop([\"prop\"], axis=1)\n",
    "elif \"mturk-clean\":\n",
    "    # assign numerical labels\n",
    "    num_of_labels = len(list(set(adf[annotations_column])))\n",
    "    if num_of_labels == 2:\n",
    "        adf[\"label\"] = [1 if x == \"b\" else 0 for x in adf[annotations_column]]\n",
    "    else:\n",
    "        print(f\"Wrong number of labels: {number_of_labels}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating proportion of rows annotated as beliefs \n",
    "anns = adf[annotations_column]\n",
    "b_count = list(anns).count(\"b\")\n",
    "\n",
    "# percentage of sentences annotated as beliefs (among all annotated)\n",
    "float(b_count)/len(adf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load negative (triggerless, automatically extracted) examples \n",
    "ndf = pd.DataFrame() \n",
    "for file in listdir(triggerless_sample):\n",
    "    f_path = os.path.join(triggerless_sample, file)\n",
    "    if f_path.endswith(\".tsv\"):\n",
    "        temp_df = pd.read_csv(f_path, sep='\\t', usecols = neg_sample_meaningful_columns)\n",
    "        ndf = pd.concat([ndf, temp_df])\n",
    "    \n",
    "ndf[\"sentence\"] = [s.strip().replace(\"\\t\", \" \").replace(\"\\n\", \" \") for s in ndf[\"sentence\"]]\n",
    "ndf = ndf.drop_duplicates(subset = [\"sentence\"])\n",
    "ndf[\"quality_controlled\"] = [\"n\"] * len(ndf)\n",
    "ndf[\"label\"] = [0] * len(ndf)\n",
    "len(ndf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many times more triggerless data to use compared to the number annotated beliefs\n",
    "# pick the number that is either the amount we want based on the multiplier or if that number is higher than the number\n",
    "# of available examples, just use all triggerless examples available\n",
    "if resample_triggerless:\n",
    "    n_neg_examples_to_use = min(b_count * neg_example_multiplier, len(ndf))\n",
    "    n_neg_examples_to_use\n",
    "    print(\"Num of triggerless sentences to sample: \", n_neg_examples_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the neg example sample, write it to a file for record keeping, and read it back in\n",
    "if resample_triggerless:\n",
    "    sample_file_name = os.path.join(triggerless_samples_used, f\"triggerless_sample_{neg_example_multiplier}_times_belief_number.tsv\")\n",
    "    if path.exists(sample_file_name) and not rewrite_sample:\n",
    "        print(\"exists\")\n",
    "        ndf = pd.read_csv(sample_file_name, sep=\"\\t\")\n",
    "    else:\n",
    "        print(\"new sample\")\n",
    "        ndf = ndf.sample(n=n_neg_examples_to_use, random_state = random_seed).reset_index(drop=True).to_csv(sample_file_name, index=False, sep=\"\\t\")\n",
    "        ndf = pd.read_csv(sample_file_name, sep=\"\\t\")\n",
    "\n",
    "    print(f\"N triggerless examples: {len(ndf)}\")\n",
    "else:\n",
    "    print(f\"Using full ndf (n={len(ndf)}) loaded from {triggerless_sample}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotated + sampled triggerless\n",
    "df = pd.concat([adf, ndf])#.reset_index(drop=True)\n",
    "print(len(df))\n",
    "# shuffle df\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "print(len(df))\n",
    "print(f\"Annotated + sampled = {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index = [x for x in range(0,len(df.index))]\n",
    "\n",
    "if setting == \"marked-trigger\":\n",
    "    # Adding markers to trigger\n",
    "    print(\"Setting: \", setting)\n",
    "    for i in df.index: \n",
    "        if (not pd.isna(df.at[i,\"trigger\"])): \n",
    "            triggerText = df.at[i,\"trigger\"]\n",
    "            orig_span = df.at[i, mention_span_column]\n",
    "            updated_span = orig_span.replace(triggerText, \"<t>\" + triggerText + \"</t>\")\n",
    "            df.at[i,\"sentence\"] = df.at[i,\"sentence\"].replace(orig_span, updated_span)\n",
    "            df.at[i,\"paragraph\"] = df.at[i,\"paragraph\"].replace(orig_span, updated_span)\n",
    "else:\n",
    "    print(\"Else setting: \", setting)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to see full cell content\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(transformer_name)\n",
    "if training_type == \"full-model\":\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(transformer_name, num_labels=2)\n",
    "else:\n",
    "    # the model will be loaded for each fold in the cross-validation condition\n",
    "    print(f\"Training type: {training_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch['sentence'], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    y_true = eval_pred.label_ids\n",
    "    y_pred = np.argmax(eval_pred.predictions, axis=-1)\n",
    "    report = metrics.classification_report(y_true, y_pred)\n",
    "    print(\"report: \\n\", report)\n",
    "    \n",
    "    print(\"rep type: \", type(report))\n",
    "    \n",
    "\n",
    "    return {'f1':metrics.f1_score(y_true, y_pred)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for creating cross-validation folds\n",
    "def get_sample_based_on_idx(data, indeces):\n",
    "    return data.iloc[indeces, :].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just checking labels are correct\n",
    "0 in set(df[\"label\"]) and 1 in set(df[\"label\"]) and len(list(set(df[\"label\"]))) == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use sentences as text to base classification on\n",
    "df[\"text\"] = df[\"sentence\"]\n",
    "# how much of the data to use (can limit number for debugging)\n",
    "df = df[:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# just checking the df looks right\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining hyperparams\n",
    "num_epochs = 20\n",
    "batch_size = 16\n",
    "weight_decay = 0.01\n",
    "if training_type == \"CV\":\n",
    "    n_folds = 5\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=os.path.join(\"./checkpoints\", specific_dir),  \n",
    "    log_level='error',\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    weight_decay=weight_decay,\n",
    "    load_best_model_at_end=True, # this is supposed to make sure the best model is loaded by the trainer at the end\n",
    "    metric_for_best_model=\"eval_f1\" \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = os.path.join(logs, specific_dir) + \".txt\"\n",
    "print(\"Log saved at: \", log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = open(log_file, \"w\") # start writing the file, don't add to existing\n",
    "\n",
    "if training_type == \"CV\":\n",
    "    print(f\"Training type: {training_type}\")\n",
    "    fold = 0\n",
    "    kfold = KFold(n_splits=n_folds, shuffle=True, random_state=random_seed)\n",
    "    for train_df_idx, eval_df_idx in kfold.split(df):\n",
    "\n",
    "        print(\"FOLD: \", fold)\n",
    "        output.write(f\"FOLD: {fold}\\n\")\n",
    "        new_df = pd.DataFrame()\n",
    "\n",
    "        train_df = get_sample_based_on_idx(df, train_df_idx)\n",
    "        print(\"LEN TRAIN DF: \", len(train_df))\n",
    "        output.write(f\"LEN TRAIN DF: {len(train_df)}\\n\")\n",
    "        \n",
    "        eval_df = get_sample_based_on_idx(df, eval_df_idx)\n",
    "        print(\"LEN EVAL: \", len(eval_df))\n",
    "        output.write(f\"LEN EVAL: {len(eval_df)}\\n\")\n",
    "        \n",
    "        ds = DatasetDict()\n",
    "        ds['train'] = Dataset.from_pandas(train_df)\n",
    "        ds['validation'] = Dataset.from_pandas(eval_df)\n",
    "        \n",
    "        train_ds = ds['train'].map(\n",
    "            tokenize, batched=True,\n",
    "            remove_columns=['index'] + meaningful_columns\n",
    "        )\n",
    "        \n",
    "        eval_ds = ds['validation'].map(\n",
    "            tokenize,\n",
    "            batched=True,\n",
    "            remove_columns=['index']+ meaningful_columns\n",
    "        )\n",
    "\n",
    "\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(transformer_name, num_labels=2)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(transformer_name)\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            compute_metrics=compute_metrics,\n",
    "            train_dataset=train_ds,\n",
    "            eval_dataset=eval_ds,\n",
    "            tokenizer=tokenizer,\n",
    "        )\n",
    "        \n",
    "        trainer.train()\n",
    "        \n",
    "        # after training, make predictions \n",
    "        preds = trainer.predict(eval_ds)\n",
    "        final_preds = [np.argmax(x) for x in preds.predictions]\n",
    "        real_f1 = metrics.f1_score(eval_df[\"label\"], final_preds)\n",
    "        real_p = metrics.precision_score(eval_df[\"label\"], final_preds)\n",
    "        real_r = metrics.recall_score(eval_df[\"label\"], final_preds)\n",
    "        \n",
    "        print(\"P: \", real_p)\n",
    "        output.write(f\"P: {real_p}\\n\")\n",
    "        print(\"R: \", real_r)\n",
    "        output.write(f\"R: {real_r}\\n\")\n",
    "        print(\"F-1: \", real_f1)\n",
    "        output.write(f\"F-1: {real_f1}\\n\")\n",
    "        \n",
    "        # save each model fold into a separate dir\n",
    "        model_name = f\"{transformer_name}-best-of-fold-{fold}-f1-{real_f1}\"\n",
    "        model_dir = os.path.join(models_dir, model_name)\n",
    "\n",
    "        trainer.save_model(model_dir)\n",
    "        \n",
    "        output.write(\"Training Log:\\n\")\n",
    "        for obj in trainer.state.log_history:\n",
    "            print(obj)\n",
    "            output.write(str(obj))\n",
    "        \n",
    "        # get false pos and neg\n",
    "        count_f_n = 0\n",
    "        count_f_p = 0\n",
    "        for i, item in enumerate(final_preds):\n",
    "            if not item == eval_ds[\"label\"][i]:\n",
    "                false_df = pd.DataFrame()\n",
    "                false_df[\"sentence\"] = [eval_df[\"sentence\"][i]]\n",
    "                false_df[\"real\"] = [eval_df[\"label\"][i]]\n",
    "                false_df[\"predicted\"] = [item]\n",
    "                new_df = pd.concat([new_df, false_df])\n",
    "                if item == 0:\n",
    "                    count_f_n += 1\n",
    "\n",
    "                else:\n",
    "                    count_f_p += 1\n",
    "\n",
    "        print(f\"n of false pos: {count_f_p}\")\n",
    "        output.write(f\"n of false pos: {count_f_p}\\n\")\n",
    "        print(f\"n of false neg: {count_f_n}\")\n",
    "        output.write(f\"n of false neg: {count_f_n}\\n\")\n",
    "\n",
    "        # write false predictions to file for error analysis\n",
    "        new_df.to_csv(os.path.join(models_dir, \"false_predictions_\" + str(fold) + \".tsv\"), sep=\"\\t\")  \n",
    "        fold += 1\n",
    "        \n",
    "elif training_type == \"full-model\":\n",
    "    \n",
    "    print(f\"Training type: {training_type}\")\n",
    "\n",
    "    train_df, eval_df = sklearn.model_selection.train_test_split(df, test_size=0.2, random_state=random_seed)\n",
    "    \n",
    "    # save eval sample \n",
    "    eval_df.to_csv(os.path.join(models_dir, \"eval_from_full_model_training.tsv\"), sep=\"\\t\")\n",
    "    \n",
    "\n",
    "    print(\"LEN TRAIN DF: \", len(train_df))\n",
    "    output.write(f\"LEN TRAIN DF: {len(train_df)}\\n\")\n",
    "    print(\"LEN EVAL: \", len(eval_df))\n",
    "    output.write(f\"LEN EVAL: {len(eval_df)}\\n\")\n",
    "    ds = DatasetDict()\n",
    "    ds['train'] = Dataset.from_pandas(train_df)\n",
    "    ds['validation'] = Dataset.from_pandas(eval_df)\n",
    "    train_ds = ds['train'].map(\n",
    "        tokenize, batched=True,\n",
    "        remove_columns=meaningful_columns#, 'sentence', 'trigger', 'annotation: b (belief or attitude), n (not a belief and not an attitude)', 'paragraph'],\n",
    "    )\n",
    "    eval_ds = ds['validation'].map(\n",
    "        tokenize,\n",
    "        batched=True,\n",
    "        remove_columns=meaningful_columns#, 'sentence', 'trigger', 'annotation: b (belief or attitude), n (not a belief and not an attitude)', 'paragraph'],\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        compute_metrics=compute_metrics,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=eval_ds,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    trainer.train()\n",
    "    # after training, make predictions (with the best model)\n",
    "    preds = trainer.predict(eval_ds)\n",
    "    final_preds = [np.argmax(x) for x in preds.predictions]\n",
    "    real_f1 = metrics.f1_score(eval_df[\"label\"], final_preds)\n",
    "    print(\"F-1: \", real_f1)\n",
    "    output.write(f\"F-1: {real_f1}\\n\")\n",
    "    model_name = f\"{transformer_name}-best-f1-{real_f1}\"\n",
    "    model_dir = os.path.join(models_dir, model_name)\n",
    "\n",
    "    trainer.save_model(model_dir)\n",
    "    new_df = pd.DataFrame()\n",
    "\n",
    "    count_f_n = 0\n",
    "    count_f_p = 0\n",
    "    for i, item in enumerate(final_preds):\n",
    "        if not item == eval_ds[\"label\"][i]:\n",
    "            false_df = pd.DataFrame()\n",
    "            false_df[\"sentence\"] = [list(eval_df[\"sentence\"])[i]]\n",
    "            false_df[\"real\"] = [list(eval_df[\"label\"])[i]]\n",
    "            false_df[\"predicted\"] = [item]\n",
    "            new_df = pd.concat([new_df, false_df])\n",
    "            if item == 0:\n",
    "                count_f_n += 1\n",
    "\n",
    "            else:\n",
    "                count_f_p += 1\n",
    "\n",
    "    print(f\"n of false pos: {count_f_p}\")\n",
    "    output.write(f\"n of false pos: {count_f_p}\\n\")\n",
    "    print(f\"n of false neg: {count_f_n}\")\n",
    "    output.write(f\"n of false neg: {count_f_n}\\n\")\n",
    "    new_df.to_csv(os.path.join(models_dir, \"false_predictions.tsv\"), sep=\"\\t\")  \n",
    "\n",
    "    \n",
    "else:\n",
    "    print(f\"Unknown training setting: {training_type}\")\n",
    "        \n",
    "        \n",
    "torch.cuda.memory_summary(device=None, abbreviated=False)\n",
    "output.write(\"{torch.cuda.memory_summary(device=None, abbreviated=False)}\")\n",
    "\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"--- {float(time.time() - start_time)/60} minutes ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
