{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from transformers.models.bert.modeling_bert import BertModel, BertPreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "import sys\n",
    "import json\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "date = datetime.datetime.now()\n",
    "year = date.year\n",
    "month = date.month\n",
    "day = date.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checking_significance = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_data = \"\"\n",
    "triggerless_sample = \"\"\n",
    "tested_model_dir = \"\"\n",
    "if checking_significance:\n",
    "    baseline_model_dir = \"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_setting = \"unmarked-trigger\" # marked-trigger or unmarked-trigger\n",
    "tested_setting = \"unmarked-trigger\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed=22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_name = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(transformer_name)\n",
    "if checking_significance:\n",
    "    baseline_model = AutoModelForSequenceClassification.from_pretrained(baseline_model_dir, num_labels=2)\n",
    "    \n",
    "tested_model = AutoModelForSequenceClassification.from_pretrained(tested_model_dir, num_labels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting some vars\n",
    "meaningful_columns = [\"paragraph\", \"sentence\", \"approx_span\", \"trigger\", \"quality_controlled\"]\n",
    "annotations_column = \"quality_controlled\"\n",
    "neg_sample_meaningful_columns = [\"sentence\", \"paragraph\"]\n",
    "mention_span_column = \"approx_span\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load annotated data\n",
    "adf = pd.DataFrame()\n",
    "for file in listdir(annotated_data):\n",
    "    print(file)\n",
    "    if file.endswith(\"tsv\"):\n",
    "        f_path = os.path.join(annotated_data, file)\n",
    "        temp_df = pd.read_csv(f_path, sep='\\t', usecols = meaningful_columns).dropna()\n",
    "        print(len(temp_df))\n",
    "        adf = pd.concat([adf, temp_df])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf[\"sentence\"] = [s.strip() for s in adf[\"sentence\"]]\n",
    "adf = adf.drop_duplicates(subset = [\"sentence\", \"approx_span\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(adf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anns = adf[annotations_column]\n",
    "b_count = list(anns).count(\"b\")\n",
    "\n",
    "# percentage of sentences annotated as beliefs (among all annotated)\n",
    "float(b_count)/len(adf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load negative examples sampled\n",
    "ndf = pd.DataFrame() \n",
    "for file in listdir(triggerless_sample):\n",
    "    f_path = os.path.join(triggerless_sample, file)\n",
    "    print(f_path)\n",
    "    if f_path.endswith(\".tsv\"):\n",
    "        temp_df = pd.read_csv(f_path, sep='\\t', usecols = neg_sample_meaningful_columns)\n",
    "        ndf = pd.concat([ndf, temp_df])\n",
    "    \n",
    "ndf[\"sentence\"] = [s.strip().replace(\"\\t\", \" \").replace(\"\\n\", \" \") for s in ndf[\"sentence\"]]\n",
    "ndf = ndf.drop_duplicates(subset = [\"sentence\"])\n",
    "ndf[\"quality_controlled\"] = [\"n\"] * len(ndf)\n",
    "len(ndf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotated + sampled triggerless\n",
    "df = pd.concat([adf, ndf])#.reset_index(drop=True)\n",
    "print(f\"Annotated + sampled = {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotated + sampled triggerless for baseline model\n",
    "if checking_significance:\n",
    "    baseline_df = pd.concat([adf, ndf])#.reset_index(drop=True)\n",
    "    print(f\"Annotated + sampled = {len(baseline_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_count = list(df[\"quality_controlled\"]).count(\"b\")\n",
    "\n",
    "# percentage of sentences annotated as beliefs\n",
    "float(b_count)/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to see full cell content\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch['sentence'], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_test_data(orig_df, setting):\n",
    "    df = orig_df\n",
    "\n",
    "    df.index = [x for x in range(0,len(df.index))]\n",
    "\n",
    "    if setting == \"marked-trigger\":\n",
    "    # Adding markers to trigger\n",
    "        print(\"Marking triggers\")\n",
    "        for i in df.index: \n",
    "            if (not pd.isna(df.at[i,\"trigger\"])): \n",
    "                triggerText = df.at[i,\"trigger\"]\n",
    "                orig_span = df.at[i, mention_span_column]\n",
    "                updated_span = orig_span.replace(triggerText, \"<t>\" + triggerText + \"</t>\")\n",
    "                df.at[i,\"sentence\"] = df.at[i,\"sentence\"].replace(orig_span, updated_span)\n",
    "                df.at[i,\"paragraph\"] = df.at[i,\"paragraph\"].replace(orig_span, updated_span)\n",
    "\n",
    "\n",
    "    # assign numerical labels\n",
    "    num_of_labels = len(list(set(df[annotations_column])))\n",
    "    if num_of_labels == 2:\n",
    "        df['label'] = np.array([1 if x == \"b\" else 0 for x in df[annotations_column]])\n",
    "    else:\n",
    "        print(f\"Wrong number of labels: {number_of_labels}\")\n",
    "        \n",
    "    ds = DatasetDict()\n",
    "    ds['test'] = Dataset.from_pandas(df)\n",
    "    test_ds = ds['test'].map(\n",
    "        tokenize,\n",
    "        batched=True,\n",
    "        remove_columns=meaningful_columns\n",
    "    )\n",
    "    return test_ds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# just checking the df looks right\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    y_true = eval_pred.label_ids\n",
    "    y_pred = np.argmax(eval_pred.predictions, axis=-1)\n",
    "    report = metrics.classification_report(y_true, y_pred)\n",
    "    print(\"report: \\n\", report)\n",
    "    return {'f1':metrics.f1_score(y_true, y_pred)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining hyperparams\n",
    "batch_size = 16\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./exp_results\", \n",
    "    log_level='error',\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tested_model_trainer = Trainer(\n",
    "    model=tested_model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if checking_significance:\n",
    "    baseline_trainer = Trainer(\n",
    "        model=baseline_model,\n",
    "        args=training_args,\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=tokenizer,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tested_setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tested_model_test_ds = prep_test_data(df, tested_setting)\n",
    "tested_preds = tested_model_trainer.predict(tested_model_test_ds)\n",
    "tested_model_final_preds = [np.argmax(x) for x in tested_preds.predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if checking_significance:\n",
    "    baseline_model_test_ds = prep_test_data(baseline_df, baseline_setting)\n",
    "    baseline_preds = baseline_trainer.predict(baseline_model_test_ds)\n",
    "    baseline_model_final_preds = [np.argmax(x) for x in baseline_preds.predictions]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tested_f1s = []\n",
    "baseline_f1s = []\n",
    "\n",
    "all_runs = 10000\n",
    "tested_model_better = 0\n",
    "\n",
    "ps = []\n",
    "rs = []\n",
    "\n",
    "# how many times my model is better than the baseline \n",
    "for i in range(all_runs):\n",
    "    indices = np.random.randint(len(tested_model_final_preds), size=len(tested_model_final_preds))\n",
    "    \n",
    "    \n",
    "    # take sample of gold labels, tested model, and, if checking significance, a baseline model\n",
    "    labels_sample = np.take(tested_model_test_ds[\"label\"], indices)\n",
    "    final_preds_sample = np.take(tested_model_final_preds, indices)\n",
    "\n",
    "    if checking_significance:\n",
    "        baseline_preds_sample = np.take(baseline_model_final_preds, indices)\n",
    "\n",
    "    # cal f1 scores for the two models\n",
    "    f1 = metrics.f1_score(labels_sample, final_preds_sample)\n",
    "    if checking_significance:\n",
    "        baseline_f1 = metrics.f1_score(labels_sample, baseline_preds_sample)\n",
    "        \n",
    "        if f1 > baseline_f1:\n",
    "            tested_model_better += 1\n",
    "\n",
    "    # calculate other stats for the tested model    \n",
    "    p = metrics.precision_score(labels_sample, final_preds_sample)\n",
    "    r = metrics.recall_score(labels_sample, final_preds_sample)\n",
    "    tested_f1s.append(f1)\n",
    "    ps.append(p)\n",
    "    rs.append(r)\n",
    "    \n",
    "\n",
    "if checking_significance:\n",
    "    print(tested_model_better)\n",
    "    prop_tested_model_better = float(tested_model_better)/all_runs\n",
    "    print(prop_tested_model_better)\n",
    "    print(\"p-value: \", round(1-prop_tested_model_better, 2))\n",
    "\n",
    "f1s_arr = np.array(tested_f1s)\n",
    "mean = np.mean(f1s_arr)\n",
    "std = np.std(f1s_arr)\n",
    "\n",
    "ps_arr = np.array(ps)\n",
    "p_mean = np.mean(ps_arr)\n",
    "p_std = np.std(ps_arr)\n",
    "\n",
    "r_arr = np.array(rs)\n",
    "r_mean = np.mean(r_arr)\n",
    "r_std = np.std(r_arr)\n",
    "\n",
    "print(\"p: \", p_mean, \" +- \",p_std )\n",
    "print(\"r: \", r_mean, \" +- \",r_std )\n",
    "print(\"f1: \", mean, \" +- \",std )\n",
    "\n",
    "latex_output = \"\"+str(round(p_mean,2))+\"\\\\textsubscript{\\\\textpm \"+str(round(p_std,2))+\"} & \"+str(round(r_mean,2))+\"\\\\textsubscript{\\\\textpm \"+str(round(r_std,2))+\"} & \"+str(round(mean,2))+\"\\\\textsubscript{\\\\textpm \"+str(round(std,2))+\"}\"\n",
    "print(\"Latex table output: \",latex_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
